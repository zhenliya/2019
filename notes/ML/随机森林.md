http://www.elecfans.com/d/647463.html
# 工作原理
随机森林是一种有监督学习算法。所构建的“森林”是决策树的集成，大部分时候都是用“bagging”方法训练的。 bagging方法，即bootstrap aggregating，采用的是随机有放回的选择训练数据然后构造分类器，最后组合学习到的模型来增加整体的效果。

随机森林分类器使用所有的决策树分类器以及bagging 分类器的超参数来控制整体结构。

随机森林算法中树的增长会给模型带来额外的随机性。与决策树不同的是，每个节点被分割成最小化误差的最佳特征，在随机森林中我们选择随机选择的特征来构建最佳分割。因此，当您在随机森林中，仅考虑用于分割节点的随机子集，甚至可以通过在每个特征上使用随机阈值来使树更加随机，而不是如正常的决策树一样搜索最佳阈值。这个过程产生了广泛的多样性，通常可以得到更好的模型。
# 决策树与随机森林的区别
如果将带有特征和标签的训练数据集输入到决策树中，它将制定一些规则集，用于预测。
决策树通常通过计算信息增益和基尼指数来生成节点和规则时，相比之下，随机森林则是随机的
另一个区别是“深度”决策树往往会遭遇过拟合问题。而随机森林则可以通过创建随机的特征子集并使用这些子集构建较小的树，随后组成子树，这种方法可以防止大部分情况的过拟合。要注意的是，这同时会使得计算速度变慢，并取决于随机森林构建的树数。


# 机器学习算法之随机森林算法重要的超参数
sklearn中的参数
## 提高模型预测准确性
- n_estimators:超参数表示算法在进行最大投票或采取预测平均值之前建立的树数。 一般来说，树的数量越多，性能越好，预测也越稳定，但这也会减慢计算速度

- max_features :它表示随机森林在单个树中可拥有的特征最大数量
- min_sample_leaf

## 加快模型计算速度
n_jobs: 超参数表示引擎允许使用处理器的数量。 若值为1，则只能使用一个处理器。 值为-1则表示没有限制。
random_state: 表示随机数种子，保证模型的输出具有可复制性。 当它被赋于一个指定值，且模型训练具有相同的参数和相同的训练数据时，该模型将始终产生相同的结果。
oob_score: 它是一种随机森林交叉验证方法。在这个抽样中，大约三分之一的数据不用于模型训练，而用来评估模型的性能。这些样本被称为袋外样本。它与留一法交叉验证方法非常相似，但几乎没有附加的计算负担

# 优缺点分析
随机森林的一个优点是它可以用于回归和分类任务，并且很容易查看模型的输入特征的相对重要性。
随机森林同时也被认为是一种非常方便且易于使用的算法，因为它是默认的超参数通常会产生一个很好的预测结果。超参数的数量也不是那么多，而且它们所代表的含义直观易懂。
机器学习中的一个重大问题是过拟合，但大多数情况下这对于随机森林分类器而言不会那么容易出现。因为只要森林中有足够多的树，分类器就不会过度拟合模型。
随机森林同时可以处理许多不同属性的特征类型，如二元的，类别的和数值的
随机森林的主要限制在于使用大量的树会使算法变得很慢，并且无法做到实时预测。一般而言，这些算法训练速度很快，预测十分缓慢。越准确的预测需要越多的树，这将导致模型越慢。
随机森林是一种预测性建模工具，而不是一种描述性工具。也就是说，如果您正在寻找关于数据中关系的描述